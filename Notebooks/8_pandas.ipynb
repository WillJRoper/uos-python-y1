{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating data in CSV files\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "By the end of this notebook you should:\n",
    "\n",
    "- Be familiar with CSV files.\n",
    "- Be able to load them with the `csv` package.\n",
    "- Be familiar with the `pandas` package.\n",
    "- Be able to perform simple manipulation of pandas DataFrames.\n",
    "- Be able to make intermediate level plots (including adding error bars).\n",
    "\n",
    "We now have *almost* all the moving parts to start analysing data in Python. The only missing ingredient is a method to load real data without typing it all out into lists! There's a myriad of different data formats we can inteface with Python, some of which we will cover later in the course (of course). \n",
    "\n",
    "In this notebook we'll utilise CSV files. These are Comma Separated Values files. Essentially tables of values with each row on a new line and each value split by a \",\". CSV files are a standard format widely used for small datasets due to their simplicity. You will be familiar with spreadsheets in Excel, Excel can both read CSV files and write spreadsheets to them. In fact, at their root, spreadsheets really are just CSV files. Here is a really simple example of a CSV file just so you can see the structure.\n",
    "\n",
    "```\n",
    "Student No., Mark, Group\n",
    "0, 70, 1\n",
    "1, 65, 1\n",
    "2, 80, 2\n",
    "3, 60, 2\n",
    "```\n",
    "\n",
    "As you can see, they quickly become difficult to parse by eye. Below we'll learn how to load these files, make them more readable, and manipulate the data within them to make plots and get results. In future PIP Lab sessions you'll be able to take your data in Excel and then save it to a CSV for analysis in Python.\n",
    "\n",
    "Before you continue, you will need to download the `PythonExampleCSVfile1.csv`` file in order to complete the exercises below. The file can be found on Canvas in \"The Python Notebooks\" and in the GitHub repo, if you cloned it you already have the files in the \"data\" directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV files\n",
    "\n",
    "Navigate to the CSV file you downloaded (or the one in repo) and double click it. This will open the file in your spreadsheet software (probably Excel but could be Numbers on Mac). You can now see the sort of information it contains. This file was generated by a PhD student, and contains real Astronomy research data - however, for these exercises, the meaning of the data is less important than the methods for accessing and manipulating it.\n",
    "\n",
    "### Specifying Directories\n",
    "\n",
    "Just a quick aside on directory paths before we start using them:\n",
    "\n",
    "As an example, when on university computers, the file path to a file in the Downloads, Desktop, or Documents directories are, respectively:\n",
    "\n",
    "```\n",
    "C://Users/<insert username>/Downloads/<insert filename>\n",
    "C://Users/<insert username>/Desktop/<insert filename>\n",
    "C://Users/<insert username>/Documents/<insert filename>\n",
    "```\n",
    "\n",
    "These are the \"address\" the file. You can point to anywhere on your computer from anywhere by defining \"absolute\" paths like this. An absolute path contains the full address of a file from the root directory (here `Users`). There are also relative paths. These are paths that start from where you currently are. For instance the relative path of `file2.txt` in `Documents` from `Downloads` would be `../Documents/file2.txt`, where the `../` moves \"up a directory\". Look at the paths above to understand this, both `Documents` and `Downloads` are in the same parent directory so we first need to move \"up from `Downloads` and then into `Documents`.\n",
    "\n",
    "This will be similar on your personal computers but not exactly the same. Have a look at the filepath displayed for the file, and then copy this into the path variable as shown above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The csv package\n",
    "\n",
    "Python has a built in module for accessing the data in CSV files. We will cover this here but in the future you'll likely use the more robust `pandas` module we'll cover shortly. To import the `csv`` module we'll use the techniques covered in [the packages notebook](6_packages.ipynb).\n",
    "\n",
    "Just to drive it home: Remember that you should always load your modules at the start of your code (i.e  in the first Jupyter cell) and only load a given module once. In these notebooks I'll break a lot of rules for the sake of demonstration. \n",
    "\n",
    "Below I will use the \"data\" directory in the repo in all examples since that is where I have stored the data. **You will need to modify these paths to point to where you have the data stored.** I will use `../` to move \"up a directory\" as described above, since the data directory is in the same parent directory as the \"Notebooks\" directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../data/PythonExampleCSVfile1.csv', 'r') as myfile:\n",
    "\n",
    "    # Assign the data in the file to a variable\n",
    "    mydata = csv.reader(myfile)\n",
    "    \n",
    "    # Loop over rows in the dataset\n",
    "    for myrow in mydata:\n",
    "        print (myrow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you didn't get the directory right in the cell above you will have seen a new type of Error like below (with <path_to_file> replaced with the path you tried).\n",
    "```\n",
    "IOError: [Errno 2] No such file or directory: <path_to_file>\n",
    "```\n",
    "\n",
    "There are quite a few moving parts in the above code, so lets go through it step by step.\n",
    "\n",
    "- The `with` statement is similar to an `if` statement or `for` loop, in that you need to indent blocks of code underneath it to tell Python what should be done with the `with`. Think of the `with` statement as meaning *'with this item do the following`*, where the \"item\" above is the CSV file stored in the `myfile` variable and the \"following\" is the code block indented underneath. Normally the item is a file like we have here, because we want to make sure we close a file when we are done with it. A `with` does this automatically so we don't need to worry about it.\n",
    "\n",
    "- The `open` command, unsurpirsingly, opens a file (this works for any file not just CSVs). `open` takes a string argument for the file path (but note that you could use a variable that has been set to the filename). The `\"r\"` that comes after the filename defines the *mode* of the file. In this case the `\"r\"` mode tells Python to read the file. If we had chosen `\"w\"` that would tell Python to \"write\" to the file, but don't do that... it'll overwrite the data in the file!\n",
    "\n",
    "- The `as myfile` assigns the opened file returned by `open` to the variable `myfile`.\n",
    "\n",
    "- The next line in the code block uses the `csv.reader` function to read the data from the file and stores it in the variable `mydata``. \n",
    "\n",
    "- Then a `for` loop is used to loop over each row of the CSV file, assigning each row to the `myrow` variable and printing the row with `print(myrow)`. Just to be absolutely clear (and drive home an earlier point from [the variables section of notebook 1](1_the_basics.ipynb): The variable names `myfile`, `mydata``, and `myrow`) are irrelevant, they are just names, e.g. even if you changed the code to say `mycolumn`, the data would still be read in as rows. \n",
    "\n",
    "- After the `for` loop is finished, the with block has been completed. You now cannot try to read in the `mydata` variable, as file has been closed when the `with` block was exited. Feel free to modify the cell above to check this. The only way you can develop an intuition for errors and what they mean about your code is by seeing them first hand and finding the cause and solution. \n",
    "\n",
    "To be able to use all the data within the CSV file, we have to assign the data to lists. The contents of the file (\"PythonExampleCSVfile1.csv\") is summarised in the table below.\n",
    "\n",
    "| Column      | Description                   |\n",
    "|-------------|-------------------------------|\n",
    "| Column A:   | Name                          |\n",
    "| Column B:   | Redshift                      |\n",
    "| Column C:   | Mean-$x$                      |\n",
    "| Column D:   | Mean-$x$ minus delta-$x$      |\n",
    "| Column E:   | Mean-$x$ plus delta-$x$       |\n",
    "| Column F:   | Mean-$y$                      |\n",
    "| Column G:   | Mean-$y$ minus delta-$y$      |\n",
    "| Column H:   | Mean-$y$ plus delta-$y$       |\n",
    "\n",
    "Now, the use of the `csv` package requires you to iterate through each row and column and assign the data to lists explictly... This is computationally expensive to do when the files get large (~GB), and tedious at best... I've included it here for completeness and to introduce the `with` statement which is actually useful. Fortunately for us there is a much better way to load data into Python! \n",
    "\n",
    "## The `pandas` Package\n",
    "\n",
    "Pronounced just like the adorable black and white bears, this module is one of the easiest ways to start playing around with data in Python. Lets start by loading the CSV file into a pandas DataFrame (think of a DataFrame exactly like a spreadsheet in Excel, Numbers, or Google Sheets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "        \n",
    "# Open the csv file as a DataFrame\n",
    "df = pd.read_csv(\"../data/PythonExampleCSVfile1.csv\", index_col=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above code imports the `pandas` package and the load the CSV file into a variable. The standard abbreviation for `pandas` is `pd`, similar to how `numpy` is always abbreviated to `np`.\n",
    "\n",
    "To read in the CSV file, we use the aptly named `read_csv` method. This method can take only one argument, the CSV filepath, however, we should also tell `pandas` not to read the first column as an index column. (In fact, `read_csv` can take a ridiculous number of arguments. Feel free to Google it and look at the docs.) Think of the index column as the row numbers in Excel, pandas allows both `int` and `str` indices, but for now we will tell `pandas` that our file doesn't have an index column already by passing `index_col=None`, `pandas` will create one for us using integers from 0 to the number of rows in the csv. \n",
    "\n",
    "Note: `pandas` will make an index column by default, however (time for another general rule of thumb) it is incredibly useful for readability if this argument is explicitly given. This is true of any argument to any function, even if using the default value it can be very helpful for understanding to explictly state the arguments.\n",
    "\n",
    "### An aside: Keyword arguments\n",
    "\n",
    "You may have noticed something different above. Here we haven't just passed an argument, we've named a variable and set it equal to a value (`index_col=None`). This is called a \"keyword argument\". A keyword argument is an argument with a default value. We can define these in functions as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strght_line_with_keyword(x, m, c=0):\n",
    "    \n",
    "    # Return the equation of a straight line\n",
    "    return m * x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined a straight line equation where the intercept (`c`) is by default 0. We can call this function specifying `c` or simply taking the default without stating `c` at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(strght_line_with_keyword(10, 2.5, c=5))\n",
    "print(strght_line_with_keyword(10, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be extremely useful. You can have as many keyword arguments as you like but unlike a normal argument (specifically called \"positional arguments\") we can pass any number of the keyword arguments. This is how complex functions like `pandas.read_csv` handle having lots of different optional arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Anyway, back to `pandas` and DataFrames. A bonus of using `pandas` is that it assumes the first row in the csv are the column names and automatically uses them. We can see these using the `columns` attribute of the Dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to print a DataFrame we can simply print the variable containing it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, this is a rare instance where not including a print gives us a nicer output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this is specific to a Jupyter notebook! Jupyter notebooks and `pandas` work extremely well together. Outside of a notebook we can't get the same sort of formatted table.\n",
    "\n",
    "\n",
    "We can also take a look at only the top $N$ rows of the DataFrame using the `head`` function (here $N=10$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with `pandas`\n",
    "\n",
    "Now that we have our data in a DataFrame we're going to want to do something with it. For instance, if we want to plot the \"Mean_X\" and \"Mean_y\" columns against each other, we can pass them to `matplotlib`. Dataframes come with some nice syntax for accessing columns of data using a `.`, as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up plot\n",
    "plt.figure()\n",
    "\n",
    "# Plot a scatter of the mean x and y data\n",
    "plt.scatter(df.Mean_X, df.Mean_y, marker=\".\")\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel(\"Mean $X$\")\n",
    "plt.ylabel(\"Mean $Y$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `pandas` has a nice \"helper function\" to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"Mean_X\", y=\"Mean_y\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have passed the function the name of the columns we want to plot on the x and y axes and told it to plot a `\"scatter\"` plot via the kind argument. \n",
    "\n",
    "But those labels are nasty, lets clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"Mean_X\", y=\"Mean_y\", kind=\"scatter\", xlabel=\"Mean $X$\", ylabel=\"Mean $Y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we want a grid too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"Mean_X\", y=\"Mean_y\", kind=\"scatter\", xlabel=\"Mean $X$\", ylabel=\"Mean $Y$\", grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we're missing a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"Mean_X\", y=\"Mean_y\", kind=\"scatter\", xlabel=\"Mean $X$\", ylabel=\"Mean $Y$\", grid=True, title=\"My Pandas plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really nice interface to quickly get plots out of a DataFrame but, in my opinion, the moment you try to do anything more complex than a simple plot like this its nicer to use the `matplotlib` interface itself rather than this `pandas` helper function. I feel it gives you more control and the code is more readable. However, if you look at [the docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) for `Dataframe.plot` you'll see there are tons of possible arguments to control a myriad of plot properties. You can really get into the weeds and make complex plots if you wanted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing data in the DataFrame \n",
    "\n",
    "We've already seen above you can get the data stored in certain columns with `.<column_name>` syntax but there are more ways to access the data. \n",
    "\n",
    "If we want to just get the values from a column and store them in a list we can do the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x_lst = df.Mean_X.values.tolist()\n",
    "print(mean_x_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `.values` gets all the values in the column `Mean_X` (you may remember this as the same syntax used in a dictionary), and `tolist()` packages those values as a `list`.\n",
    "\n",
    "If we want to grab a specific row within the DataFrame, we can use the `loc` attribute, which uses the index. In our case the index is just the row number (starting at 0), but strings are allowed if you set up your DataFrame like that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first row of data\n",
    "print(df.loc[0])\n",
    "print()  # create a blank line in output\n",
    "\n",
    "# Print the 51st row of data\n",
    "print(df.loc[50])\n",
    "print()  # create a blank line in output\n",
    "\n",
    "# Print the Mean_X in the 51st row\n",
    "print(df.loc[50,\"Mean_X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loc` attribute also allows slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0:2,\"Mean_X\"])\n",
    "print(df.loc[0:2,\"Mean_X\"].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we have again used `.values.tolist()` to store `\"Mean_X\"` from the first 3 in a list.\n",
    "\n",
    "I encourage you to use the `help` function (or [the docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)) to look through `pd.DataFrame` and see all the other attributes and methods which can be used. There's a pleathora! In particular, the `shape` attribute can be used to tell you how many rows, and how many columns are in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The output, `(84,8)`, tells us that our `DataFrame` (`df`) has 84 rows and 8 columns. Note that this means we actually have 84 rows, not 83 rows and the columns names (header), i.e. the column names **do not** count as a row.\n",
    "\n",
    "## Adding columns to a DataFrame\n",
    "\n",
    "One particularly useful thing we can do is compute things from the data and add this new information to the DataFrame. You can apply any operator to any columns, and `pandas` will perform the operation on every value in that column (in reality this is done row by row behind the scenes). You can then store the results back into the DataFrame in the same way as you would store a value in a dictionary with a key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new columns containing the data produced by an operation\n",
    "df[\"Mean_X_upper\"] = df.Mean_x_plus_delta - df.Mean_X\n",
    "df[\"Mean_X_lower\"] = df.Mean_X - df.Mean_x_minus_delta\n",
    "\n",
    "# Output the first row to see these changes\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Saving your DataFrame\n",
    "\n",
    "Now that we have added some new data to the DataFrame we want a way to save the DataFrame including this new data. To do so we can write the DataFrame back to a new CSV. This way, if you screw up, you can load back in the data from the saved state. For this course, you will rarely be working on sufficiently large data to warrant such an approach, but it is useful.\n",
    "\n",
    "To save a CSV file we can simply call the `to_csv` method. Remember to change this path to point to where you have stored the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/Week6_with_errors.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises 8.1\n",
    "\n",
    "Using the dataframe we've loaded above:\n",
    "\n",
    "1. Write code to view and create columns in a DataFrame.\n",
    "    - Print the 50th, 72nd and 9th rows, including all their columns.\n",
    "    - In one code line, print the \"Mean_y\" values for rows 62-71.\n",
    "    - Add two new columns to your DataFrame similar to the \"Mean_X_upper\" and \"Mean_X_lower\" example above but now for \"Mean_y\".\n",
    "    - Save the updated DataFrame with the new columns to a csv file.\n",
    "    - Read the csv you just created back into a new DataFrame, and have a look - what do you now notice? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
